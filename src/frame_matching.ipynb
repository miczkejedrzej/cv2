{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading the video, extracting, first frame \n",
    "here the video is loaded, and the first frame extracted, the frame used later for the homography transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fps rate in the recorded video is 30.013\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "video = cv2.VideoCapture(r\"../videos/resized_gameplay_3.mp4\")\n",
    "original_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f'the fps rate in the recorded video is {original_fps}')\n",
    "\n",
    "# Check if the video file opened successfully\n",
    "if not video.isOpened():\n",
    "    print(\"Error opening the video file\")\n",
    "    exit()\n",
    "\n",
    "# Display the first frame\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        # Wait for user input\n",
    "        key = cv2.waitKey(0)  # Wait indefinitely for a key press\n",
    "        # if key == ord('f'):  # Close the window if 'f' is pressed\n",
    "        break\n",
    "    else:\n",
    "        print(\"Unable to read the video frame.\")\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the canny detector, and hugh lines tried to manually extract valuable features from the frame, but it doesnt seem promising the keypoints seem much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "frame_grayscale =  cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "frame_gray = frame_grayscale.copy()\n",
    "#parameters for 256x256\n",
    "#corners = cv2.goodFeaturesToTrack(frame_grayscale, maxCorners=50, qualityLevel=0.1, minDistance=100)\n",
    "corners = cv2.goodFeaturesToTrack(frame_grayscale, maxCorners=4, qualityLevel=0.1, minDistance=200)\n",
    "corners = np.intp(corners)\n",
    "\n",
    "# Draw corners\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv2.circle(frame_grayscale, (x, y), 5, 255, -1)\n",
    "\n",
    "cv2.imshow('Corners', frame_grayscale)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "edges = cv2.Canny(frame_gray, 50, 150)\n",
    "#for 256 x 256\n",
    "#lines = cv2.HoughLines(edges, 1, np.pi / 180, 100)\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "\n",
    "# Draw detected lines\n",
    "for rho, theta in lines[:, 0]:\n",
    "    a, b = np.cos(theta), np.sin(theta)\n",
    "    x0, y0 = a * rho, b * rho\n",
    "    x1 = int(x0 + 1000 * (-b))\n",
    "    y1 = int(y0 + 1000 * a)\n",
    "    x2 = int(x0 - 1000 * (-b))\n",
    "    y2 = int(y0 - 1000 * a)\n",
    "    cv2.line(frame_gray, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow('Lines', frame_gray)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# video.release()\n",
    "\n",
    "reference_pick = cv2.imread(\"../images/board_image.jpg\")\n",
    "reference_pick= cv2.resize(reference_pick, (876,876), interpolation = cv2.INTER_AREA)\n",
    "reference_pick = cv2.cvtColor(reference_pick,cv2.COLOR_BGR2GRAY)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key points finding  and matching them\n",
    "\n",
    "here , I try to find the keypoints on both pictures and match them, to be able to now the transformation between the reference image and the video, thus mapp coordinates or compute the homographic transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matches(bf, kp1, desc1, kp2, desc2, num_matches=30):\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    pts_ref = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts_frame = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    H, _ = cv2.findHomography(pts_frame, pts_ref, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp video frame to align with reference image\n",
    "    aligned_frame = cv2.warpPerspective(frame, H, (reference_pick.shape[1], reference_pick.shape[0]))\n",
    "    \n",
    "    matches = cv2.drawMatches(\n",
    "        reference_pick,\n",
    "        kp1,\n",
    "        frame,\n",
    "        kp2,\n",
    "        matches[:num_matches],\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "    )\n",
    "    cv2.imshow(\"matches\",matches)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.imshow('Aligned Frame', aligned_frame)\n",
    "    cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "sift = cv2.SIFT_create()  # kp + desc\n",
    "fast = cv2.FastFeatureDetector_create()  # only kp\n",
    "star = cv2.xfeatures2d.StarDetector_create()  # only kp\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # only desc\n",
    "\n",
    "\n",
    "# Detect and compute keypoints and descriptors\n",
    "reference_pick = cv2.imread(\"../images/board_image.jpg\")\n",
    "reference_pick = cv2.resize(reference_pick, (512,512), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "orb_kp1, orb_des1 = orb.detectAndCompute(reference_pick, None)\n",
    "orb_kp2, orb_des2 = orb.detectAndCompute(frame, None)\n",
    "\n",
    "sift_kp1, sift_des1 = sift.detectAndCompute(reference_pick, None)\n",
    "sift_kp2, sift_des2 = sift.detectAndCompute(frame, None)\n",
    "\n",
    "fast_kp1 = fast.detect(reference_pick, None)\n",
    "fast_kp2 = fast.detect(frame, None)\n",
    "\n",
    "star_kp1 = star.detect(reference_pick, None)\n",
    "star_kp2 = star.detect(frame, None)\n",
    "\n",
    "_, fast_brief_des1 = brief.compute(reference_pick, fast_kp1)\n",
    "_, fast_brief_des2 = brief.compute(frame, fast_kp2)\n",
    "\n",
    "_, star_brief_des1 = brief.compute(reference_pick, star_kp1)\n",
    "_, star_brief_des2 = brief.compute(frame, star_kp2)\n",
    "\n",
    "\n",
    "# Match features using BFMatcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "bf_l2 = cv2.BFMatcher(cv2.NORM_L2,crossCheck=True)\n",
    "\n",
    "\n",
    "show_matches(bf_l2,sift_kp1,sift_des1,sift_kp2,sift_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf,orb_kp1,orb_des1,orb_kp2,orb_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf_l2,orb_kp1,orb_des1,orb_kp2,orb_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf,fast_kp1,fast_brief_des1,fast_kp2,fast_brief_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf_l2,fast_kp1,fast_brief_des1,fast_kp2,fast_brief_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf,star_kp1,star_brief_des1,star_kp2,star_brief_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "show_matches(bf_l2,star_kp1,star_brief_des1,star_kp2,star_brief_des2,50)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1,3,6 worth looking at\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contour detection\n",
    "The idea is to , before finding the keypoints, find the biggest contour on the frame from the video, in order to search for the keypoints only within this contour - which corresponds to the board isolated from the -environment - i.e the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_pick = cv2.imread(\"../images/board_image.jpg\")\n",
    "reference_pick = cv2.resize(reference_pick, (512,512), interpolation = cv2.INTER_AREA)\n",
    "reference_pick = cv2.cvtColor(reference_pick,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"original frame\",frame)\n",
    "cv2.waitKey(0)\n",
    "frame_gray =cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"before_clahe\",frame_gray)\n",
    "cv2.waitKey(0)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "frame_gray = clahe.apply(frame_gray)\n",
    "cv2.imshow(\"after_clahe\",frame_gray)\n",
    "cv2.waitKey(0)\n",
    "frame_gray = cv2.GaussianBlur(frame_gray,(3,3),0)\n",
    "cv2.imshow(\"after_gaussian\",frame_gray)\n",
    "cv2.waitKey(0)\n",
    "_,thresh = cv2.threshold(frame_gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "largest_contour = max(contours, key=cv2.contourArea)\n",
    "cv2.drawContours(frame_gray, [largest_contour], -1, (0, 255, 0), 3)\n",
    "\n",
    "\n",
    "cv2.imshow('Largest Contour', frame_gray)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "empty_image = np.zeros_like(frame_gray)\n",
    "cv2.drawContours(empty_image, [largest_contour], -1, (255, 255, 255), 3)\n",
    "cv2.imshow('Largest Contour', empty_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keypoints detection using the extracted mask, restricting the area of the keypoints search in the frame\n",
    "\n",
    "having the mask we can search for the keypoints only within it , which  decreases the chance of wrongfull matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(frame_gray, dtype=np.uint8)\n",
    "\n",
    "# Fill the largest contour on the mask\n",
    "cv2.drawContours(mask, [largest_contour], -1, 255, -1)  # Fill the contour with white\n",
    "cv2.imshow(\"mask\",mask)\n",
    "cv2.waitKey(0)\n",
    "orb = cv2.ORB_create()\n",
    "sift = cv2.SIFT_create()  # kp + desc\n",
    "fast = cv2.FastFeatureDetector_create()  # only kp\n",
    "star = cv2.xfeatures2d.StarDetector_create()  # only kp\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # only desc\n",
    "\n",
    "\n",
    "# Detect and compute keypoints and descriptors\n",
    "reference_pick = cv2.imread(\"../images/board_image.jpg\")\n",
    "reference_pick = cv2.resize(reference_pick, (512,512), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "\n",
    "orb_kp1, orb_des1 = orb.detectAndCompute(reference_pick, None)\n",
    "orb_kp2, orb_des2 = orb.detectAndCompute(frame, mask)\n",
    "sift_kp1, sift_des1 = sift.detectAndCompute(reference_pick, None)\n",
    "sift_kp2, sift_des2 = sift.detectAndCompute(frame, mask)\n",
    "fast_kp1 = fast.detect(reference_pick, None)\n",
    "fast_kp2 = fast.detect(frame, mask)\n",
    "check = fast.detect(frame, None)\n",
    "star_kp1 = star.detect(reference_pick, None)\n",
    "star_kp2 = star.detect(frame, mask)\n",
    "\n",
    "# Compute descriptors for FAST and STAR keypoints\n",
    "_, fast_brief_des2 = brief.compute(frame, fast_kp2)\n",
    "_, star_brief_des2 = brief.compute(frame, star_kp2)\n",
    "\n",
    "# Match features using BFMatcher (as before)\n",
    "show_matches(bf_l2, sift_kp1, sift_des1, sift_kp2, sift_des2, 50)\n",
    "show_matches(bf, orb_kp1, orb_des1, orb_kp2, orb_des2, 50)\n",
    "show_matches(bf_l2, orb_kp1, orb_des1, orb_kp2, orb_des2, 50)\n",
    "show_matches(bf, fast_kp1, fast_brief_des1, fast_kp2, fast_brief_des2, 50)\n",
    "show_matches(bf_l2, fast_kp1, fast_brief_des1, fast_kp2, fast_brief_des2, 50)\n",
    "show_matches(bf, star_kp1, star_brief_des1, star_kp2, star_brief_des2, 50)\n",
    "show_matches(bf_l2, star_kp1, star_brief_des1, star_kp2, star_brief_des2, 50)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering the image based on the rgb/hsv/hls features, which may be usefull to extract later the board from the frame in the vido, and only then do the keypoint analysis and  transform /match\n",
    "The colorFinder class enables to manually find the range of values for the features that interest us and filter those values in image, thus potentially enabling to distinguish the board from the background \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ColorFinder:\n",
    "    def __init__(self, trackBar=False, colorSpace='HSV'):\n",
    "        \"\"\"\n",
    "        :param trackBar: Whether to use OpenCV trackbars to dynamically adjust HSV/HLS/GBR values. Default is False.\n",
    "        :param colorSpace: The color space to use, either 'HSV', 'HLS', or 'GBR'. Default is 'HSV'.\n",
    "        \"\"\"\n",
    "        self.trackBar = trackBar\n",
    "        self.colorSpace = colorSpace.upper()  # Convert to uppercase for consistency\n",
    "        if self.trackBar:\n",
    "            self.initTrackbars()\n",
    "\n",
    "    def empty(self, a):\n",
    "        pass\n",
    "\n",
    "    def initTrackbars(self):\n",
    "        \"\"\"Initialize the OpenCV trackbars for dynamic HSV/HLS/GBR value adjustment.\"\"\"\n",
    "        cv2.namedWindow(\"TrackBars\")\n",
    "        cv2.resizeWindow(\"TrackBars\", 640, 240)\n",
    "        \n",
    "\n",
    "        if self.colorSpace == 'HLS':\n",
    "            cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Light Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Light Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "        elif self.colorSpace == 'GBR':\n",
    "            # Add trackbars for Green, Blue, and Red channels\n",
    "            cv2.createTrackbar(\"Green Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Green Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Blue Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Blue Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Red Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Red Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "        else:\n",
    "            cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Value Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Value Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "\n",
    "    def getTrackbarValues(self):\n",
    "        \"\"\"Get the current color values set by the trackbars.\"\"\"\n",
    "       \n",
    "\n",
    "        if self.colorSpace == 'HLS':\n",
    "            hmin = cv2.getTrackbarPos(\"Hue Min\", \"TrackBars\")\n",
    "            smin = cv2.getTrackbarPos(\"Sat Min\", \"TrackBars\")\n",
    "            hmax = cv2.getTrackbarPos(\"Hue Max\", \"TrackBars\")\n",
    "            smax = cv2.getTrackbarPos(\"Sat Max\", \"TrackBars\")\n",
    "            lmin = cv2.getTrackbarPos(\"Light Min\", \"TrackBars\")\n",
    "            lmax = cv2.getTrackbarPos(\"Light Max\", \"TrackBars\")\n",
    "            colorVals = {\"hmin\": hmin, \"smin\": smin, \"lmin\": lmin,\n",
    "                         \"hmax\": hmax, \"smax\": smax, \"lmax\": lmax}\n",
    "        elif self.colorSpace == 'GBR':\n",
    "            gmin = cv2.getTrackbarPos(\"Green Min\", \"TrackBars\")\n",
    "            gmax = cv2.getTrackbarPos(\"Green Max\", \"TrackBars\")\n",
    "            bmin = cv2.getTrackbarPos(\"Blue Min\", \"TrackBars\")\n",
    "            bmax = cv2.getTrackbarPos(\"Blue Max\", \"TrackBars\")\n",
    "            rmin = cv2.getTrackbarPos(\"Red Min\", \"TrackBars\")\n",
    "            rmax = cv2.getTrackbarPos(\"Red Max\", \"TrackBars\")\n",
    "            colorVals = {\"gmin\": gmin, \"bmin\": bmin, \"rmin\": rmin,\n",
    "                         \"gmax\": gmax, \"bmax\": bmax, \"rmax\": rmax}\n",
    "        else:\n",
    "            hmin = cv2.getTrackbarPos(\"Hue Min\", \"TrackBars\")\n",
    "            smin = cv2.getTrackbarPos(\"Sat Min\", \"TrackBars\")\n",
    "            hmax = cv2.getTrackbarPos(\"Hue Max\", \"TrackBars\")\n",
    "            smax = cv2.getTrackbarPos(\"Sat Max\", \"TrackBars\")\n",
    "            vmin = cv2.getTrackbarPos(\"Value Min\", \"TrackBars\")\n",
    "            vmax = cv2.getTrackbarPos(\"Value Max\", \"TrackBars\")\n",
    "            colorVals = {\"hmin\": hmin, \"smin\": smin, \"vmin\": vmin,\n",
    "                         \"hmax\": hmax, \"smax\": smax, \"vmax\": vmax}\n",
    "\n",
    "        return colorVals\n",
    "\n",
    "    def update(self, img, myColor=None):\n",
    "        \"\"\"Find a specified color in the given image.\"\"\"\n",
    "        imgColor = []\n",
    "        mask = []\n",
    "\n",
    "        if self.trackBar:\n",
    "            myColor = self.getTrackbarValues()\n",
    "\n",
    "        if myColor is not None:\n",
    "            # If using GBR, directly use the image as it is in GBR format (OpenCV loads images as BGR)\n",
    "            if self.colorSpace == 'HLS':\n",
    "                imgColorSpace = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "            elif self.colorSpace == 'GBR':\n",
    "                imgColorSpace = img  # No conversion needed, as we are working with BGR (GBR in OpenCV terms)\n",
    "            else:  # HSV\n",
    "                imgColorSpace = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Apply the color filtering for GBR\n",
    "            if self.colorSpace == 'GBR':\n",
    "                lower = np.array([myColor['gmin'], myColor['bmin'], myColor['rmin']])\n",
    "                upper = np.array([myColor['gmax'], myColor['bmax'], myColor['rmax']])\n",
    "            elif self.colorSpace == 'HLS':\n",
    "                lower = np.array([myColor['hmin'], myColor['lmin'], myColor['smin']])\n",
    "                upper = np.array([myColor['hmax'], myColor['lmax'], myColor['smax']])\n",
    "            else:  # HSV\n",
    "                lower = np.array([myColor['hmin'], myColor['smin'], myColor['vmin']])\n",
    "                upper = np.array([myColor['hmax'], myColor['smax'], myColor['vmax']])\n",
    "\n",
    "            # Create a mask and apply it to the image\n",
    "            mask = cv2.inRange(imgColorSpace, lower, upper)\n",
    "            imgColor = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return imgColor, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hmin': 17, 'smin': 0, 'vmin': 0, 'hmax': 179, 'smax': 255, 'vmax': 255}\n",
      "{'gmin': 102, 'bmin': 0, 'rmin': 0, 'gmax': 255, 'bmax': 255, 'rmax': 255}\n",
      "{'hmin': 0, 'smin': 0, 'lmin': 33, 'hmax': 179, 'smax': 255, 'lmax': 255}\n"
     ]
    }
   ],
   "source": [
    "def find_values(image,colorspace='gbr'):\n",
    "    if colorspace == 'hsv':\n",
    "         image = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "    elif colorspace == 'hls':\n",
    "         image = cv2.cvtColor(image,cv2.COLOR_BGR2HLS)\n",
    "    image = cv2.GaussianBlur(image,(5,5),0)\n",
    "    if colorspace == 'hls':\n",
    "        colorFinder = ColorFinder(trackBar=True,colorSpace='HLS')\n",
    "    elif colorspace == 'gbr':\n",
    "         colorFinder = ColorFinder(trackBar=True,colorSpace='GBR')\n",
    "    else:\n",
    "        colorFinder = ColorFinder(trackBar=True)\n",
    "    while True:\n",
    "            # Get the current color values from trackbars\n",
    "            hsvVals = colorFinder.getTrackbarValues()\n",
    "\n",
    "            # Update the image with the new HSV values\n",
    "            imgProcessed, mask = colorFinder.update(image, hsvVals)\n",
    "\n",
    "            # Display the original image and the processed image\n",
    "            cv2.imshow(\"Original Image\", image)\n",
    "            cv2.imshow(\"Processed Image\", imgProcessed)\n",
    "\n",
    "            # Exit if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Release resources\n",
    "    cv2.destroyAllWindows()\n",
    "    return hsvVals\n",
    "    \n",
    "def show_filtered_values(image,vals,colorspace='gbr'):\n",
    "    if colorspace == 'hsv':\n",
    "        image= cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "        colorFinder = ColorFinder(trackBar=False,colorSpace='HSV')\n",
    "    elif colorspace == 'hls':\n",
    "        image= cv2.cvtColor(image,cv2.COLOR_BGR2HLS)\n",
    "        colorFinder = ColorFinder(trackBar=False,colorSpace='HLS')\n",
    "    else:\n",
    "        colorFinder = ColorFinder(trackBar=False,colorSpace='GBR')   \n",
    "    # colorFinder = ColorFinder(trackBar=False)\n",
    "    imgProcessed, mask = colorFinder.update(image, vals)\n",
    "    cv2.imshow(\"filtered image\",imgProcessed)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return \n",
    "\n",
    "'''one can play around with the parameters, but so far only hsv seemed usefull somehow?'''\n",
    "\n",
    "'''usage, in the hsv space'''\n",
    "found_vals = find_values(frame,colorspace=\"hsv\")\n",
    "print(found_vals)\n",
    "show_filtered_values(frame,found_vals,colorspace='hsv')\n",
    "'''usage, in the gbr space, default'''\n",
    "# so maybe we can manually find the color of the table, and  filter out this color on the image , interesting idea \n",
    "found_vals = find_values(frame)\n",
    "print(found_vals)\n",
    "show_filtered_values(frame,found_vals)\n",
    "'''usage ,in the hls space'''\n",
    "found_vals = find_values(frame,colorspace='hls')\n",
    "print(found_vals)\n",
    "show_filtered_values(frame,found_vals,colorspace='hls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
