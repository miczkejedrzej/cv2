{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from shapely.geometry import Polygon, box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "before the detections can take place the raw video must be preprocessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the auxillary function responsible  for alligning the video frame to the reference frame to infer the coordinates\n",
    "of the grid'''\n",
    "def show_matches(bf, kp1, desc1, kp2, desc2,reference_pick,frame ,num_matches=30):\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    pts_ref = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts_frame = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    H, _ = cv2.findHomography(pts_frame, pts_ref, cv2.RANSAC, 5.0)\n",
    "    aligned_frame = cv2.warpPerspective(frame, H, (reference_pick.shape[1], reference_pick.shape[0]))\n",
    "    \n",
    "    \n",
    "\n",
    "    matches = cv2.drawMatches(\n",
    "        reference_pick,\n",
    "        kp1,\n",
    "        frame,\n",
    "        kp2,\n",
    "        matches[:num_matches],\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "    )\n",
    "    return H, aligned_frame\n",
    "\n",
    "'''usefull function to get the frid tiles coordinates forom the stored annotations - annotations were stored in format \n",
    "such that for each polygon 4 corners stored and for circle the center and 4 radii in perpendicular directions'''\n",
    "def draw_polygons(image, annotations, color=(0, 255, 0)):\n",
    "    for key, polygons in annotations.items():\n",
    "        for polygon in polygons:\n",
    "            pts = np.array(polygon, dtype=np.int32)\n",
    "            if len(pts) ==4:\n",
    "                cv2.polylines(image, [pts], isClosed=True, color=color, thickness=2)\n",
    "            elif len(pts) ==5:\n",
    "                center_coordinates = (pts[0][0],pts[0][1])\n",
    "                rads = pts[1:]-pts[0]\n",
    "                rads = np.abs(rads)\n",
    "                rads = np.max(rads,axis=1)\n",
    "                rads = np.mean(rads)\n",
    "                radius=int(rads)\n",
    "                cv2.circle(image, center_coordinates, radius, color, thickness=2)\n",
    "\n",
    "\n",
    "'''function used in order to transform the annotations made on the reference frame to be able to make\n",
    "use of them on the video frame , \n",
    "params:\n",
    "annotations - the premade annotations stored in the file, \n",
    "H - the transformation matrix between the reference frame and the given frame \n",
    " '''\n",
    "def transform_annotations(annotations, H):\n",
    "    transformed_annotations = {}\n",
    "    for key, polygons in annotations.items():\n",
    "        transformed_annotations[key] = []\n",
    "        for polygon in polygons:\n",
    "            if not polygon:  \n",
    "                continue\n",
    "            pts = np.array(polygon, dtype=np.float32).reshape(-1, 1, 2)\n",
    "            transformed_pts = cv2.perspectiveTransform(pts, H)\n",
    "            transformed_annotations[key].append(transformed_pts.reshape(-1, 2).tolist())\n",
    "    return transformed_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frame allignmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video__to_allign_1 = cv2.VideoCapture('../videos/easy_1.mp4')\n",
    "video__to_allign_2 = cv2.VideoCapture('../videos/easy_2.mp4')\n",
    "video__to_allign_3 = cv2.VideoCapture('../videos/easy_3.mp4')\n",
    "video__to_allign_4 = cv2.VideoCapture('../videos/medium_1.mp4')\n",
    "video__to_allign_5 = cv2.VideoCapture('../videos/medium_2.mp4')\n",
    "video__to_allign_6 = cv2.VideoCapture('../videos/medium_3.mp4')\n",
    "video__to_allign_7 = cv2.VideoCapture('../videos/hard_1.mp4')\n",
    "video__to_allign_8 = cv2.VideoCapture('../videos/hard_2.mp4')\n",
    "video__to_allign_9 = cv2.VideoCapture('../videos/hard_3.mp4')\n",
    "\n",
    "\n",
    "'''function returning the given frame of the video usefull for computing the transformation matrix  between the video \n",
    "and the annotated reference pick'''\n",
    "def give_frame(video):\n",
    "    while True:\n",
    "        ret_2, frame_to_allign = video.read()\n",
    "        cv2.imshow('Frame', frame_to_allign)\n",
    "        if frame_to_allign is None:\n",
    "            break\n",
    "\n",
    "        key = cv2.waitKey(30)\n",
    "        if key == ord('q' ):\n",
    "            break\n",
    "    # cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return frame_to_allign\n",
    "\n",
    "'''the allign frames  displayed'''\n",
    "frame_to_allign_1 = give_frame(video__to_allign_1)\n",
    "frame_to_allign_2 =  give_frame(video__to_allign_2)\n",
    "frame_to_allign_3 = give_frame(video__to_allign_3)\n",
    "frame_to_allign_4 = give_frame(video__to_allign_4)\n",
    "frame_to_allign_5 = give_frame(video__to_allign_5)\n",
    "frame_to_allign_6 = give_frame(video__to_allign_6)\n",
    "frame_to_allign_7 = give_frame(video__to_allign_7)\n",
    "frame_to_allign_8 = give_frame(video__to_allign_8)\n",
    "frame_to_allign_9 = give_frame(video__to_allign_9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation reading\n",
    "\n",
    "here the reading of the annotation takes place, the annotation were made on the reference board image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue_start': [[[280, 594], [280, 554], [323, 557], [320, 594]]],\n",
       " 'grid_1': [[[281, 553], [281, 516], [317, 514], [321, 555]]],\n",
       " 'grid_2': [[[279, 516], [280, 476], [320, 476], [319, 516]]],\n",
       " 'grid_3': [[[281, 476], [281, 438], [319, 436], [319, 476]]],\n",
       " 'grid_4': [[[280, 476], [241, 474], [241, 437], [280, 437]]],\n",
       " 'grid_5': [[[240, 475], [201, 475], [203, 436], [241, 436]]],\n",
       " 'grid_6': [[[201, 436], [203, 398], [243, 399], [243, 434]]],\n",
       " 'grid_7': [[[203, 399], [203, 358], [243, 359], [243, 400]]],\n",
       " 'grid_8': [[[201, 398], [164, 398], [164, 360], [203, 358]]],\n",
       " 'grid_9': [[[162, 397], [125, 398], [125, 360], [165, 360]]],\n",
       " 'grid_10': [[[123, 398], [85, 398], [85, 359], [123, 360]]],\n",
       " 'grid_11': [[[85, 359], [85, 321], [124, 319], [125, 358]]],\n",
       " 'red_start': [[[85, 320], [83, 279], [127, 281], [123, 318]]],\n",
       " 'grid_12': [[[123, 318], [162, 319], [164, 280], [125, 280]]],\n",
       " 'grid_13': [[[162, 318], [202, 317], [203, 282], [163, 282]]],\n",
       " 'grid_14': [[[201, 318], [242, 319], [242, 281], [202, 280]]],\n",
       " 'grid_15': [[[203, 280], [202, 242], [241, 241], [242, 282]]],\n",
       " 'grid_16': [[[201, 240], [203, 202], [243, 201], [241, 240]]],\n",
       " 'grid_17': [[[242, 240], [280, 240], [281, 203], [243, 200]]],\n",
       " 'grid_18': [[[279, 242], [320, 241], [320, 203], [281, 202]]],\n",
       " 'grid_19': [[[279, 202], [280, 162], [319, 162], [322, 202]]],\n",
       " 'grid_20': [[[281, 160], [281, 122], [320, 125], [320, 163]]],\n",
       " 'grid_21': [[[281, 121], [281, 84], [321, 85], [321, 123]]],\n",
       " 'grid_22': [[[323, 124], [358, 124], [360, 84], [321, 84]]],\n",
       " 'green_start': [[[360, 85], [397, 85], [399, 125], [359, 124]]],\n",
       " 'grid_23': [[[360, 126], [358, 162], [398, 164], [399, 128]]],\n",
       " 'grid_24': [[[361, 164], [360, 203], [398, 202], [398, 164]]],\n",
       " 'grid_25': [[[359, 202], [357, 242], [397, 242], [399, 204]]],\n",
       " 'grid_26': [[[398, 242], [438, 242], [438, 204], [400, 202]]],\n",
       " 'grid_27': [[[437, 243], [475, 241], [477, 202], [439, 202]]],\n",
       " 'grid_28': [[[439, 244], [437, 281], [478, 280], [479, 243]]],\n",
       " 'grid_29': [[[437, 282], [437, 318], [477, 318], [477, 281]]],\n",
       " 'grid_30': [[[477, 317], [516, 319], [517, 283], [481, 280]]],\n",
       " 'grid_31': [[[517, 281], [555, 280], [555, 318], [515, 318]]],\n",
       " 'grid_32': [[[557, 319], [593, 319], [593, 281], [555, 278]]],\n",
       " 'grid_33': [[[557, 319], [555, 358], [594, 358], [595, 321]]],\n",
       " 'grid_34': [[[516, 358], [555, 356], [557, 398], [515, 396]]],\n",
       " 'grid_35': [[[477, 360], [516, 358], [514, 397], [478, 397]]],\n",
       " 'grid_36': [[[436, 358], [479, 358], [478, 396], [437, 398]]],\n",
       " 'grid_37': [[[435, 398], [479, 399], [478, 438], [439, 437]]],\n",
       " 'grid_38': [[[438, 438], [477, 438], [477, 476], [438, 474]]],\n",
       " 'grid_39': [[[399, 439], [439, 437], [439, 475], [397, 475]]],\n",
       " 'grid_40': [[[359, 434], [401, 434], [400, 477], [359, 477]]],\n",
       " 'grid_41': [[[359, 478], [401, 476], [401, 514], [361, 514]]],\n",
       " 'grid_42': [[[363, 514], [360, 554], [400, 555], [401, 516]]],\n",
       " 'grid_43': [[[362, 555], [360, 592], [399, 592], [401, 554]]],\n",
       " 'grid_44': [[[321, 594], [359, 594], [361, 554], [321, 556]]],\n",
       " 'blue_house_1': [[[340, 532],\n",
       "   [340, 516],\n",
       "   [355, 531],\n",
       "   [340, 547],\n",
       "   [323, 534]]],\n",
       " 'blue_house_2': [[[341, 492],\n",
       "   [341, 478],\n",
       "   [357, 492],\n",
       "   [341, 506],\n",
       "   [325, 494]]],\n",
       " 'blue_house_3': [[[339, 455],\n",
       "   [339, 439],\n",
       "   [355, 454],\n",
       "   [341, 468],\n",
       "   [324, 456]]],\n",
       " 'blue_house_4': [[[341, 412],\n",
       "   [340, 398],\n",
       "   [355, 415],\n",
       "   [341, 429],\n",
       "   [325, 416]]],\n",
       " 'yellow_house_1': [[[533, 342],\n",
       "   [533, 324],\n",
       "   [548, 340],\n",
       "   [535, 353],\n",
       "   [518, 341]]],\n",
       " 'yellow_house_2': [[[493, 342],\n",
       "   [492, 326],\n",
       "   [507, 338],\n",
       "   [494, 353],\n",
       "   [479, 341]]],\n",
       " 'yellow_house_3': [[[454, 341],\n",
       "   [454, 326],\n",
       "   [469, 342],\n",
       "   [455, 353],\n",
       "   [438, 340]]],\n",
       " 'yellow_house_4': [[[414, 340],\n",
       "   [413, 324],\n",
       "   [430, 340],\n",
       "   [416, 354],\n",
       "   [401, 340]]],\n",
       " 'green_house_1': [[[339, 147],\n",
       "   [339, 130],\n",
       "   [355, 146],\n",
       "   [341, 161],\n",
       "   [325, 146]]],\n",
       " 'green_house_2': [[[338, 188],\n",
       "   [338, 170],\n",
       "   [355, 186],\n",
       "   [341, 199],\n",
       "   [323, 187]]],\n",
       " 'green_house_3': [[[339, 225],\n",
       "   [339, 210],\n",
       "   [357, 225],\n",
       "   [342, 238],\n",
       "   [325, 226]]],\n",
       " 'green_house_4': [[[339, 266],\n",
       "   [339, 245],\n",
       "   [353, 265],\n",
       "   [343, 278],\n",
       "   [324, 266]]],\n",
       " 'red_house_1': [[[147, 339], [146, 324], [162, 339], [145, 352], [131, 342]]],\n",
       " 'red_house_2': [[[186, 339], [185, 322], [201, 338], [187, 353], [173, 338]]],\n",
       " 'red_house_3': [[[224, 340], [224, 324], [239, 340], [227, 354], [210, 338]]],\n",
       " 'red_house_4': [[[264, 339], [264, 322], [281, 338], [264, 354], [248, 338]]],\n",
       " 'yellow_start': [[[555, 356], [556, 398], [595, 398], [595, 358]]],\n",
       " 'red_base_2': [[[145, 132], [175, 105], [147, 76], [119, 102]]],\n",
       " 'red_base_3': [[[117, 104], [91, 76], [117, 48], [147, 77]]],\n",
       " 'red_base_4': [[[87, 130], [117, 107], [92, 78], [61, 100]]],\n",
       " 'green_base_1': [[[506, 105], [533, 76], [562, 102], [534, 133]]],\n",
       " 'green_base_2': [[[562, 104], [590, 75], [563, 50], [533, 74]]],\n",
       " 'green_base_3': [[[562, 104], [589, 129], [619, 102], [591, 78]]],\n",
       " 'green_base_4': [[[533, 133], [562, 161], [594, 129], [562, 102]]],\n",
       " 'yellow_base_1': [[[564, 521], [590, 547], [562, 576], [535, 548]]],\n",
       " 'yellow_base_2': [[[563, 576], [588, 603], [619, 575], [592, 547]]],\n",
       " 'yellow_base_3': [[[562, 574], [534, 601], [562, 630], [589, 602]]],\n",
       " 'yellow_base_4': [[[534, 600], [505, 575], [535, 546], [560, 574]]],\n",
       " 'blue_base_1': [[[119, 574], [145, 547], [173, 572], [147, 600]]],\n",
       " 'blue_base_2': [[[117, 575], [91, 547], [117, 516], [143, 543]]],\n",
       " 'blue_base_3': [[[119, 573], [92, 602], [119, 630], [147, 602]]],\n",
       " 'blue_base_4': [[[93, 603], [63, 574], [90, 545], [117, 572]]],\n",
       " 'red_base_1': [[[117, 160], [146, 133], [119, 102], [89, 130]]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "reference_pick = cv2.imread(\"../images/board_image.jpg\")\n",
    "reference_pick = cv2.resize(reference_pick, (512,512), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "with open('../resources/annotated_image.pkl', 'rb') as f:\n",
    "    loaded_annotations = pickle.load(f)\n",
    "\n",
    "with open('../resources/new_annon.pkl', 'rb') as f:\n",
    "    new_loaded_annotations = pickle.load(f)\n",
    "\n",
    "loaded_annotations = {key:val for key,val in loaded_annotations.items() if 'base' not in key}\n",
    "\n",
    "\n",
    "'''so here with this weird code , we account for the the mistakes made when annotating, some typos etc\n",
    "so auxillary fragmented annotations with corrections were made '''\n",
    "new_loaded_annotations['red_start_1'] = new_loaded_annotations['']\n",
    "new_loaded_annotations.pop('')\n",
    "new_loaded_annotations\n",
    "\n",
    "new_loaded_annotations = {\n",
    "    key.replace('start', 'base'): value\n",
    "    for key, value in new_loaded_annotations.items()\n",
    "}\n",
    "\n",
    "for key, value in new_loaded_annotations.items():\n",
    "    loaded_annotations[key] = value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''the content of the annotations showed'''\n",
    "loaded_annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The alligning frame and annotations\n",
    "\n",
    "Here the annotation take place using introduced earlier auxillary function show_matches. The function basically relies on the sift algorithm used with euclidean norm and crosscheck() , the idea is to find the 200 best keypoints and based on them allign the frames. 200 seemed as big enough number to account for the possible inaccuracies caused by occlusions of grid , shadowing and light reflection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' function alligning the frame basing on the sift\n",
    "params:\n",
    "frame - frame to allign\n",
    "reference_pick -  reference image  to allign the frame to '''\n",
    "def allign_frame(frame, reference_pick,to_show=False):\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    sift_kp1, sift_des1 = sift.detectAndCompute(reference_pick, None)\n",
    "    sift_kp2, sift_des2 = sift.detectAndCompute(frame, None)\n",
    "    bf_l2 = cv2.BFMatcher(cv2.NORM_L2,crossCheck=True)\n",
    "    H, aligned_frame = show_matches(bf_l2, sift_kp1, sift_des1, sift_kp2, sift_des2,reference_pick,frame ,200)\n",
    "    original_size = (1707*0.4, 1707*0.4)  \n",
    "    resized_size = (512, 512)\n",
    "    scaled_annotations = {}\n",
    "    for key, polygons in loaded_annotations.items():\n",
    "        scaled_annotations[key] = [\n",
    "            [[int(pt[0] * resized_size[0] / original_size[0]), int(pt[1] * resized_size[1] / original_size[1])] for pt in polygon]\n",
    "            for polygon in polygons\n",
    "        ]\n",
    "\n",
    "    transformed_annotations = transform_annotations(scaled_annotations, np.linalg.inv(H))\n",
    "    aligned_frame_with_polygons = aligned_frame.copy()\n",
    "    draw_polygons(aligned_frame_with_polygons, scaled_annotations)\n",
    "    original_frame_with_polygons = frame.copy()\n",
    "    draw_polygons(original_frame_with_polygons, transformed_annotations)\n",
    "\n",
    "    return transformed_annotations\n",
    "\n",
    "transformed_annotations_1 = allign_frame(frame_to_allign_1,reference_pick)\n",
    "transformed_annotations_2 = allign_frame(frame_to_allign_2,reference_pick)\n",
    "transformed_annotations_3 = allign_frame(frame_to_allign_3,reference_pick)\n",
    "transformed_annotations_4 = allign_frame(frame_to_allign_4,reference_pick)\n",
    "transformed_annotations_5 = allign_frame(frame_to_allign_5,reference_pick)\n",
    "transformed_annotations_6 = allign_frame(frame_to_allign_6,reference_pick)\n",
    "transformed_annotations_7 = allign_frame(frame_to_allign_7,reference_pick)\n",
    "transformed_annotations_8 = allign_frame(frame_to_allign_8,reference_pick)\n",
    "transformed_annotations_9 = allign_frame(frame_to_allign_9,reference_pick)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorfinder\n",
    "\n",
    "Here is stored the class  responsible for most of the work done concerning the detections. Basically the function filters\n",
    "the image based on the indicated color ranges. It works in 3 colorspace, HSV,HLS,GBR and in two modes: The debbug mode with trackbar=True, allows for the manual changing of the color-ranges in the course of the video, thus seeing which elements are remaning in which color mask. It was basically used to find the proper ranges for all of the elements -pawns, dice, pawns in base. and the second mode with trackbar=False , is basically filterring the video for the provided colorrange, so it can be used when the desired color range is found in the debbug mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorFinder:\n",
    "    '''params:\n",
    "    trackbar- boolean, when trackbar is set to true the debugg mode is present and the user can change the filter range of \n",
    "    the video, when false the video is run with predetermined values \n",
    "    colorspace-  the desired colorspace, since different elements can be filtered in different colorspaces, supports\n",
    "    arguments : HSV, HLS, GBR\n",
    "    red -boolean, so basially this argument is specifically designed to address the issue of the red color hue values\n",
    "     in the hsv space. The issue is that the red color can either have very high hue or very low. So if we set red to True\n",
    "      and set the min hue and max hue, the filtering for hue  instead of being [min,max] is the complement of this set so\n",
    "      [0,min] + [max,max_possible_hue]'''\n",
    "\n",
    "\n",
    "    def __init__(self, trackBar=False, colorSpace='HSV',red=False):\n",
    "    \n",
    "        self.trackBar = trackBar\n",
    "        if red:\n",
    "            self.red = True\n",
    "        else:\n",
    "            self.red = False\n",
    "        self.colorSpace = colorSpace.upper()  \n",
    "        if self.trackBar:\n",
    "            self.initTrackbars()\n",
    "\n",
    "    def empty(self, a):\n",
    "        pass\n",
    "\n",
    "    def initTrackbars(self):\n",
    "        \"\"\"Initialize the OpenCV trackbars for dynamic HSV/HLS/GBR value adjustment.\"\"\"\n",
    "        cv2.namedWindow(\"TrackBars\")\n",
    "        cv2.resizeWindow(\"TrackBars\", 640, 240)\n",
    "        \n",
    "\n",
    "        if self.colorSpace == 'HLS':\n",
    "            cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Light Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Light Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "        elif self.colorSpace == 'GBR':\n",
    "           \n",
    "            cv2.createTrackbar(\"Green Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Green Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Blue Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Blue Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Red Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Red Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "        else:\n",
    "            cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Value Min\", \"TrackBars\", 0, 255, self.empty)\n",
    "            cv2.createTrackbar(\"Value Max\", \"TrackBars\", 255, 255, self.empty)\n",
    "\n",
    "    def getTrackbarValues(self):\n",
    "        \"\"\"Get the current color values set by the trackbars.\"\"\"\n",
    "       \n",
    "\n",
    "        if self.colorSpace == 'HLS':\n",
    "            hmin = cv2.getTrackbarPos(\"Hue Min\", \"TrackBars\")\n",
    "            smin = cv2.getTrackbarPos(\"Sat Min\", \"TrackBars\")\n",
    "            hmax = cv2.getTrackbarPos(\"Hue Max\", \"TrackBars\")\n",
    "            smax = cv2.getTrackbarPos(\"Sat Max\", \"TrackBars\")\n",
    "            lmin = cv2.getTrackbarPos(\"Light Min\", \"TrackBars\")\n",
    "            lmax = cv2.getTrackbarPos(\"Light Max\", \"TrackBars\")\n",
    "            colorVals = {\"hmin\": hmin, \"smin\": smin, \"lmin\": lmin,\n",
    "                         \"hmax\": hmax, \"smax\": smax, \"lmax\": lmax}\n",
    "        elif self.colorSpace == 'GBR':\n",
    "            gmin = cv2.getTrackbarPos(\"Green Min\", \"TrackBars\")\n",
    "            gmax = cv2.getTrackbarPos(\"Green Max\", \"TrackBars\")\n",
    "            bmin = cv2.getTrackbarPos(\"Blue Min\", \"TrackBars\")\n",
    "            bmax = cv2.getTrackbarPos(\"Blue Max\", \"TrackBars\")\n",
    "            rmin = cv2.getTrackbarPos(\"Red Min\", \"TrackBars\")\n",
    "            rmax = cv2.getTrackbarPos(\"Red Max\", \"TrackBars\")\n",
    "            colorVals = {\"gmin\": gmin, \"bmin\": bmin, \"rmin\": rmin,\n",
    "                         \"gmax\": gmax, \"bmax\": bmax, \"rmax\": rmax}\n",
    "        else:\n",
    "            hmin = cv2.getTrackbarPos(\"Hue Min\", \"TrackBars\")\n",
    "            smin = cv2.getTrackbarPos(\"Sat Min\", \"TrackBars\")\n",
    "            hmax = cv2.getTrackbarPos(\"Hue Max\", \"TrackBars\")\n",
    "            smax = cv2.getTrackbarPos(\"Sat Max\", \"TrackBars\")\n",
    "            vmin = cv2.getTrackbarPos(\"Value Min\", \"TrackBars\")\n",
    "            vmax = cv2.getTrackbarPos(\"Value Max\", \"TrackBars\")\n",
    "            colorVals = {\"hmin\": hmin, \"smin\": smin, \"vmin\": vmin,\n",
    "                         \"hmax\": hmax, \"smax\": smax, \"vmax\": vmax}\n",
    "\n",
    "        return colorVals\n",
    "\n",
    "    def update(self, img, myColor=None):\n",
    "        \"\"\"Find a specified color in the given image.\"\"\"\n",
    "        imgColor = []\n",
    "        mask = []\n",
    "\n",
    "        if self.trackBar:\n",
    "            myColor = self.getTrackbarValues()\n",
    "\n",
    "        if myColor is not None:\n",
    "            if self.colorSpace == 'HLS':\n",
    "                imgColorSpace = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "            elif self.colorSpace == 'GBR':\n",
    "                imgColorSpace = img  \n",
    "            else: \n",
    "                imgColorSpace = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            \n",
    "            if self.colorSpace == 'GBR':\n",
    "                lower = np.array([myColor['bmin'],myColor['gmin'],  myColor['rmin']])\n",
    "                upper = np.array([myColor['bmax'],myColor['gmax'],  myColor['rmax']])\n",
    "            elif self.colorSpace == 'HLS':\n",
    "                lower = np.array([myColor['hmin'], myColor['lmin'], myColor['smin']])\n",
    "                upper = np.array([myColor['hmax'], myColor['lmax'], myColor['smax']])\n",
    "            else:  \n",
    "                lower = np.array([myColor['hmin'], myColor['smin'], myColor['vmin']])\n",
    "                upper = np.array([myColor['hmax'], myColor['smax'], myColor['vmax']])\n",
    "\n",
    "            if self.red and self.colorSpace == 'HSV':\n",
    "                lower_1 = np.zeros_like(lower)\n",
    "                lower_2 = np.zeros_like(lower)\n",
    "                upper_1 = np.zeros_like(lower)\n",
    "                upper_2 = np.zeros_like(lower)\n",
    "                upper_1[0] = lower[0] \n",
    "                upper_1[1:] = upper[1:]  \n",
    "                lower_1[0] = 0           \n",
    "                lower_1[1:] = lower[1:] \n",
    "                lower_2[0] = upper[0]    \n",
    "                lower_2[1:] = lower[1:]  \n",
    "                upper_2[0] = 179         \n",
    "                upper_2[1:] = upper[1:] \n",
    "                mask_1 = cv2.inRange(imgColorSpace, lower_1, upper_1)\n",
    "                mask_2 = cv2.inRange(imgColorSpace, lower_2, upper_2)\n",
    "                imgColor_1 = cv2.bitwise_and(img, img, mask=mask_1)\n",
    "                imgColor_2 = cv2.bitwise_and(img, img, mask=mask_2)\n",
    "                imgColor = cv2.bitwise_or(imgColor_1,imgColor_2)\n",
    "            else:\n",
    "                mask = cv2.inRange(imgColorSpace, lower, upper)\n",
    "                imgColor = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        return imgColor, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorfinder Test\n",
    "Here the colorfinder is tested in the debugg mode so the functionality can be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "video_2 = cv2.VideoCapture('../videos/easy_1.mp4')\n",
    "\n",
    "colorFinder = ColorFinder(trackBar=True, colorSpace='HSV')\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, frame_3= video_2.read()\n",
    "    if not success:\n",
    "        print(\"Failed to capture frame. Exiting...\")\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame_3, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "    imgColor, mask = colorFinder.update(frame_3)\n",
    "    imggray = cv2.cvtColor(imgColor,cv2.COLOR_BGR2GRAY)\n",
    "    _,img_threshed = cv2.threshold(imggray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)) \n",
    "\n",
    "    img_threshed= cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)  \n",
    "   \n",
    "   \n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_CLOSE, kernel)  \n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel) \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5)) \n",
    "    img_threshed = cv2.morphologyEx(img_threshed,cv2.MORPH_DILATE,kernel)\n",
    "    \n",
    "\n",
    "    cv2.imshow(\"Original Video\", frame_3)\n",
    "    cv2.imshow(\"Filtered Video\", imgColor)\n",
    "    cv2.imshow(\"thresholded\", img_threshed)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "    if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting the elements of the gird\n",
    "Here are the functions devouted to detecting the elements of the grid, since our logic was to filter out the respective elements by the color ranges they fall into, each element had to have separate category, these values here are already an effect of tedious work with the colorfinder in the debugg mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blue_base(frame, annotations, frame_to_annotate):\n",
    "    bounding_boxes_blue = []\n",
    "    vals_blue_start = {'rmin': 0, 'rmax': 6, \"bmin\": 45, \"bmax\": 255, 'gmax': 185, 'gmin': 0}\n",
    "    colorFinder_blue_start = ColorFinder(trackBar=False, colorSpace='GBR')\n",
    "    frame_gauss = cv2.GaussianBlur(frame, (15, 15), 0)\n",
    "    \n",
    "    # filtering the area of interest since we only search for the pawns in the blue base and blue start in this case \n",
    "    mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    annotations_for_blue_start = {key: value for key, value in annotations.items() if 'blue_base' in key or 'blue_start' in key }\n",
    "    for _, points in annotations_for_blue_start.items():\n",
    "        points_array = np.array(points, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points_array], 255)\n",
    "\n",
    "    masked_frame = cv2.bitwise_and(frame_gauss, frame_gauss, mask=mask)\n",
    "    imgColor, color_mask = colorFinder_blue_start.update(masked_frame,vals_blue_start)\n",
    "    imggray = cv2.cvtColor(imgColor, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed = cv2.threshold(imggray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    #Morphological operations \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_CLOSE, kernel)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_DILATE, kernel)\n",
    "    contours, _ = cv2.findContours(img_threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #finding contours \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        bounding_boxes_blue.append((x,y,w,h))\n",
    "        #drawing the bounding boxes for detected contours \n",
    "        cv2.rectangle(frame_to_annotate, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    return frame_to_annotate,bounding_boxes_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_base(frame,annotations,frame_to_annotate):\n",
    "    boundig_boxes_red = []\n",
    "    vals_red_start = {'hmin': 53, 'hmax': 132, 'smin': 220, 'smax': 255, 'vmin': 0, 'vmax': 118}\n",
    "    vals_red_start = {'hmin': 53, 'hmax': 132, 'smin': 220, 'smax': 255, 'vmin': 0, 'vmax': 108}\n",
    "    colorFinder_red_start = ColorFinder(trackBar=False, colorSpace='HSV', red=True)\n",
    "    frame_3_gauss = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "\n",
    "    #filtering the area of intereset since we search only in red base and red start \n",
    "    mask = np.zeros(frame_3.shape[:2], dtype=np.uint8)\n",
    "    annotations_for_red_start= {key: value for (key, value) in annotations.items() if  'red_start'in key  or 'red_base' in key}\n",
    "    for _, points in annotations_for_red_start.items():\n",
    "        points_array = np.array(points, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points_array], 255)\n",
    "    masked_frame = cv2.bitwise_and(frame_3_gauss, frame_3_gauss, mask=mask)\n",
    "    imgColor, color_mask = colorFinder_red_start.update(masked_frame, vals_red_start)\n",
    "    imggray = cv2.cvtColor(imgColor, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed = cv2.threshold(imggray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)  \n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_CLOSE, kernel)  \n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "    #Finding contours \n",
    "    contours, _ = cv2.findContours(img_threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 50:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        boundig_boxes_red.append((x,y,w,h))\n",
    "        # Draw bounding boxes for detected contours \n",
    "        cv2.rectangle(frame_to_annotate, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    return frame_to_annotate,boundig_boxes_red\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blue_pawns(frame,annotations,frame_annotated):\n",
    "    mask_blue = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    current_annotations_blue = {key: val for key, val in annotations.items() if 'grid' in key}\n",
    "    # Filtering the are of interest since we only search for the pawns on the grid \n",
    "    for polygon_name_blue, points_blue in current_annotations_blue.items():\n",
    "        points_array_blue = np.array(points_blue, dtype=np.int32)\n",
    "        cv2.fillPoly(mask_blue, [points_array_blue], 255)\n",
    "    colorFinder_blue = ColorFinder(trackBar=False, colorSpace='HSV')\n",
    "    vals_blue = {'hmin': 95, 'hmax': 112, 'smin': 140, 'smax': 255, 'vmin': 0, 'vmax': 156}\n",
    "    masked_frame_blue = cv2.bitwise_and(frame, frame, mask=mask_blue)\n",
    "    imgColor_blue, mask_blue = colorFinder_blue.update(masked_frame_blue, vals_blue)\n",
    "    imggray_blue = cv2.cvtColor(imgColor_blue, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed_blue = cv2.threshold(imggray_blue, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations \n",
    "    kernel_blue = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    img_threshed_blue = cv2.morphologyEx(img_threshed_blue, cv2.MORPH_OPEN, kernel_blue)\n",
    "    kernel_blue = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    img_threshed_blue = cv2.morphologyEx(img_threshed_blue, cv2.MORPH_CLOSE, kernel_blue)\n",
    "    img_threshed_blue = cv2.morphologyEx(img_threshed_blue, cv2.MORPH_OPEN, kernel_blue)\n",
    "    kernel_blue = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "    img_threshed_blue = cv2.morphologyEx(img_threshed_blue, cv2.MORPH_DILATE, kernel_blue)\n",
    "\n",
    "    # Contour detection \n",
    "    contours_blue, _ = cv2.findContours(img_threshed_blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes_blue = []\n",
    "    for cnt_blue in contours_blue:\n",
    "        area_blue = cv2.contourArea(cnt_blue)\n",
    "        if 50 < area_blue < 400:  \n",
    "            perimeter_blue = cv2.arcLength(cnt_blue, True)\n",
    "            if perimeter_blue == 0:\n",
    "                continue\n",
    "            circularity_blue = 4 * np.pi * (area_blue / (perimeter_blue ** 2))\n",
    "            if 0.4 < circularity_blue < 0.9:  \n",
    "                x_blue, y_blue, w_blue, h_blue = cv2.boundingRect(cnt_blue)\n",
    "                bounding_boxes_blue.append((x_blue, y_blue, w_blue, h_blue))\n",
    "                #drawing the bounding boxes for the deteced contours \n",
    "                cv2.rectangle(frame_annotated, (x_blue, y_blue), (x_blue + w_blue, y_blue + h_blue), (255, 0, 0), 2)\n",
    "    return frame_annotated,bounding_boxes_blue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_pawns(frame,annotations,frame_annotated):\n",
    "    vals_red  = {'hmin': 9, 'hmax': 165, 'smin': 180, 'smax': 255, 'vmin': 80, 'vmax': 140}\n",
    "    vals_red  = {'hmin': 9, 'hmax': 165, 'smin': 200, 'smax': 255, 'vmin': 80, 'vmax': 140}\n",
    "    colorFinder_red = ColorFinder(trackBar=False, colorSpace='HSV',red='True')\n",
    "    \n",
    "    #filtering the area of interest since we search for the red pawns only in the grid tiles \n",
    "    mask_red = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    current_annotations_red = {key: val for key, val in annotations.items() if 'red' not in key and 'base' not in key }\n",
    "    for polygon_name_red, points_red in current_annotations_red.items():\n",
    "        points_array_red = np.array(points_red, dtype=np.int32)\n",
    "        cv2.fillPoly(mask_red, [points_array_red], 255)\n",
    "    masked_frame_red = cv2.bitwise_and(frame, frame, mask=mask_red)\n",
    "    imgColor_red, mask_red = colorFinder_red.update(masked_frame_red, vals_red)\n",
    "    imggray_red = cv2.cvtColor(imgColor_red, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed_red = cv2.threshold(imggray_red, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations \n",
    "    kernel_red = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    img_threshed_red = cv2.morphologyEx(img_threshed_red, cv2.MORPH_CLOSE, kernel_red)  # Close gaps\n",
    "    img_threshed_red = cv2.morphologyEx(img_threshed_red, cv2.MORPH_OPEN, kernel_red)\n",
    "    kernel_red = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6))\n",
    "    img_threshed_red = cv2.morphologyEx(img_threshed_red, cv2.MORPH_DILATE, kernel_red)\n",
    "\n",
    "    # Contour detection \n",
    "    contours_red, _ = cv2.findContours(img_threshed_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes_red = []\n",
    "\n",
    "    for cnt_red in contours_red:\n",
    "        area_red = cv2.contourArea(cnt_red)\n",
    "        if 100 < area_red < 500:  \n",
    "            perimeter_red = cv2.arcLength(cnt_red, True)\n",
    "            if perimeter_red == 0:\n",
    "                continue\n",
    "            circularity_red = 4 * np.pi * (area_red / (perimeter_red ** 2))\n",
    "            if 0.5 < circularity_red < 0.9:  \n",
    "                x_red, y_red, w_red, h_red = cv2.boundingRect(cnt_red)\n",
    "                bounding_boxes_red.append((x_red, y_red, w_red, h_red))\n",
    "                #drawing the bounding boxes for the detected contours \n",
    "                cv2.rectangle(frame_annotated, (x_red, y_red), (x_red + w_red, y_red + h_red), (0, 0, 255), 2)\n",
    "    return frame_annotated,bounding_boxes_red\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_pips(dice_roi, circularity_threshold=0.5):\n",
    "  \n",
    "    gray_roi = cv2.cvtColor(dice_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(gray_roi, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))\n",
    "    binary_mask = cv2.morphologyEx(binary_mask,cv2.MORPH_OPEN,kernel)\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(binary_mask)\n",
    "    pip_count = 0\n",
    "    \n",
    "    #validation of contours \n",
    "    for i,contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, closed=True)\n",
    "        if perimeter > 0:  \n",
    "            circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "        else:\n",
    "            circularity = 0\n",
    "        #sophisticated hand-crafted conditions when to assess the contour as a valid dice pip candidate\n",
    "        if area > 10 or area <1 or (circularity < 0.6 and area <= 2.5):\n",
    "            continue\n",
    "        if circularity >= circularity_threshold:\n",
    "            pip_count += 1\n",
    "            # Draw the contour on the mask for visualization\n",
    "            cv2.drawContours(contour_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "    if pip_count ==0:\n",
    "        return None, binary_mask\n",
    "    return min(pip_count,6), binary_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''''only the dice detected'''\n",
    "def detect_dice(frame, frame_annotated, mask_binary_white, tracker, tracking, frame_counter):\n",
    "    colorFinder = ColorFinder(trackBar=False, colorSpace='HLS')\n",
    "    vals = {'hmin': 107, 'hmax': 140, 'smin': 12, 'smax': 255, 'lmin': 140, 'lmax': 255}\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=3, detectShadows=True)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    reset_interval =9  # Number of frames after which the tracker is reinitialized\n",
    "\n",
    "    # Background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    bg_subtracted_frame = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "\n",
    "    frame_to_track = frame.copy()\n",
    "    # Color filtering on the original frame\n",
    "    imgColor, mask = colorFinder.update(bg_subtracted_frame, vals)\n",
    "    frame_to_track, _ = colorFinder.update(frame_to_track, vals)\n",
    "    imggray = cv2.cvtColor(imgColor, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed = cv2.threshold(imggray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_CLOSE, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_DILATE, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "   \n",
    "    combined_mask = cv2.bitwise_and(img_threshed, fgmask)\n",
    "    combined_mask = cv2.bitwise_and(combined_mask, cv2.bitwise_not(mask_binary_white))\n",
    "    if tracking and frame_counter % reset_interval != 0:\n",
    "        # if tracker succesfull and not time to reset tracker than bounding box for te detected dice\n",
    "        success, bbox = tracker.update(frame_to_track)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            # Draw tracking bounding box\n",
    "            cv2.rectangle(frame_annotated, (x - 5, y - 5), (x + w + 5, y + h + 5), (255, 255, 255), 2)  # Blue for tracking\n",
    "        else:\n",
    "            # If object lost trakcing false\n",
    "            tracking = False\n",
    "            tracker = None\n",
    "\n",
    "    if not tracking or frame_counter % reset_interval == 0:\n",
    "        # Detecting the contoursagain since no tracking or the reset interval has happened \n",
    "        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Find the largest contour, as we can have only once dice at time right?\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            if cv2.contourArea(largest_contour) > 30:\n",
    "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "                # KFC Tracker initialized  on the normal color frame since , if object found we do not want it\n",
    "                # to disappear with the background subtraction\n",
    "                tracker = cv2.TrackerKCF_create()\n",
    "                tracker.init(frame_to_track, (x, y, w, h))  \n",
    "                tracking = True\n",
    "                frame_counter = 0  \n",
    "                cv2.rectangle(frame_annotated, (x - 5, y - 5), (x + w + 5, y + h + 5), (255, 255, 255), 2)  # Green for new tracking box\n",
    "        else:\n",
    "            tracking = False \n",
    "    return frame_annotated, tracker, tracking\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''the same logic as in the function above with the change that also the pips are detected\n",
    "in case the tracker is stable on an object and remain in one place, the threshold movement is small '''\n",
    "def detect_dice_with_pips(frame, frame_annotated, mask_binary_white, tracker, tracking, frame_counter, stable_count, dice_pips, prev_bbox, calculated):\n",
    "    colorFinder = ColorFinder(False, 'HLS')\n",
    "    vals = {'hmin': 107, 'hmax': 140, 'smin': 12, 'smax': 255, 'lmin': 140, 'lmax': 255}\n",
    "    dice_image = np.zeros_like(frame)\n",
    "    frame_to_count_pips = frame.copy()\n",
    "\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=3, detectShadows=True)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "    reset_interval = 7 # Number of frames after which the tracker is reinitialized\n",
    "\n",
    "    # Background subtraction and preprocessing\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    bg_subtracted_frame = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "\n",
    "    # Color filtering on the original frame\n",
    "    imgColor, mask = colorFinder.update(bg_subtracted_frame, vals)\n",
    "    #imgColor, mask = colorFinder.update(frame, vals)\n",
    "\n",
    "    imggray = cv2.cvtColor(imgColor, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed = cv2.threshold(imggray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_OPEN, kernel)\n",
    "    img_threshed = cv2.morphologyEx(img_threshed, cv2.MORPH_DILATE, kernel)\n",
    "    \n",
    "    combined_mask = cv2.bitwise_and(img_threshed, cv2.bitwise_not(mask_binary_white))\n",
    "\n",
    "\n",
    "    if tracking and frame_counter % reset_interval != 0:\n",
    "        success, bbox = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            # Draw tracking box\n",
    "            cv2.rectangle(frame_annotated, (x-5, y-5), (x + w+5, y + h+5), (255, 255, 255), 2)\n",
    "\n",
    "            # Check if the bounding box is stable for at least 5frames\n",
    "            stable_count += 1\n",
    "            if stable_count >=5 :  \n",
    "                if prev_bbox and abs(prev_bbox[0] - x) < 10 and  abs(prev_bbox[1] - y) < 10:\n",
    "                    if not calculated:\n",
    "                        if y> 5 and y+h+5< frame.shape[0] and x >5 and x+w+5< frame.shape[1]:\n",
    "                            frame_to_count_pips = frame[y-5:y+h+5, x-5:x+w+5]\n",
    "                            dice_pips, dice_image = detect_pips(frame_to_count_pips)  # Call pip detection function\n",
    "                            if dice_pips is not None:\n",
    "                                cv2.putText(frame_annotated, f\"Pips: {dice_pips}\", (x, y - 10),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                                calculated = True  \n",
    "\n",
    "                # Update prev_bbox if bounding box is stable\n",
    "            prev_bbox = (x, y, w, h)\n",
    "        else:\n",
    "            tracking = False\n",
    "            tracker = None\n",
    "            stable_count = 0\n",
    "            calculated = False  \n",
    "\n",
    "    if not tracking or frame_counter % reset_interval == 0:\n",
    "        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            if cv2.contourArea(largest_contour) > 30:\n",
    "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "                tracker = cv2.TrackerKCF_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                tracking = True\n",
    "                stable_count = 0\n",
    "            if not frame_counter % reset_interval == 0:\n",
    "                calculated = False  # Reset flag on tracker initialization\n",
    "\n",
    "    # Proximity check: Reset flag if the bounding box has moved significantly\n",
    "    if prev_bbox is not None:\n",
    "        x, y, w, h = prev_bbox\n",
    "        if abs(prev_bbox[0] - x) > 50 or abs(prev_bbox[1] - y) > 50:\n",
    "            calculated = False  \n",
    "\n",
    "    return frame_annotated, tracker, tracking, stable_count, dice_pips, dice_image, prev_bbox, calculated\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the logic identicall to detectin red blue and green only the parameters differ'''\n",
    "def detect_yellow_pawns(frame,annotations,frame_annotated):\n",
    "    bounding_boxes_yellow = []\n",
    "    mask_yellow = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    current_annotations_yellow = {key: val for key, val in annotations.items() if 'yellow'  in key or 'grid' in key or 'start' in key}\n",
    "    colorFinder_yellow = ColorFinder(trackBar=False, colorSpace='HSV')\n",
    "    vals_yellow = {'hmin': 14, 'hmax': 24, 'smin': 190, 'smax': 255, 'vmin': 0, 'vmax': 176}\n",
    "    #filtering the area of interest \n",
    "    for polygon_name_yellow, points_yellow in current_annotations_yellow.items():\n",
    "        points_array_yellow = np.array(points_yellow, dtype=np.int32)\n",
    "        cv2.fillPoly(mask_yellow, [points_array_yellow], 255)\n",
    "    masked_frame_yellow = cv2.bitwise_and(frame, frame, mask=mask_yellow)\n",
    "\n",
    "    #\n",
    "    imgColor_yellow, _ = colorFinder_yellow.update(masked_frame_yellow, vals_yellow)\n",
    "    imggray_yellow = cv2.cvtColor(imgColor_yellow, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed_yellow = cv2.threshold(imggray_yellow, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
    "    img_threshed_yellow = cv2.morphologyEx(img_threshed_yellow, cv2.MORPH_OPEN, kernel)  \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    img_threshed_yellow = cv2.morphologyEx(img_threshed_yellow, cv2.MORPH_CLOSE, kernel)  \n",
    "    img_threshed_yellow = cv2.morphologyEx(img_threshed_yellow, cv2.MORPH_OPEN, kernel)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    img_threshed_yellow = cv2.morphologyEx(img_threshed_yellow, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "    # Contour detection \n",
    "\n",
    "    contours_yellow, _ = cv2.findContours(img_threshed_yellow, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    contours_yellow = sorted(contours_yellow, key=cv2.contourArea, reverse=True)[:4]\n",
    "    for cnt in contours_yellow:\n",
    "        area_yellow = cv2.contourArea(cnt)\n",
    "        if area_yellow > 75: \n",
    "            perimeter_yellow = cv2.arcLength(cnt, True)\n",
    "            if perimeter_yellow == 0:  \n",
    "                continue\n",
    "            circularity_yellow = 4 * np.pi * (area_yellow / (perimeter_yellow ** 2))\n",
    "\n",
    "            if circularity_yellow > 0.2:  \n",
    "                x_yellow, y_yellow, w_yellow, h_yellow = cv2.boundingRect(cnt)\n",
    "                bounding_boxes_yellow.append((x_yellow, y_yellow, w_yellow, h_yellow))\n",
    "                cv2.rectangle(frame_annotated, (x_yellow, y_yellow), (x_yellow + w_yellow, y_yellow + h_yellow), (0, 255, 255), 2)\n",
    "    return frame_annotated, bounding_boxes_yellow\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the same as detectin red ,blue,yellow but  with different params'''\n",
    "def detect_green_pawns(frame,annotations,frame_annotated):\n",
    "    vals_green = {'hmin':38,'hmax':90,'smin':131,'smax':255,'vmin':67,'vmax':115}\n",
    "    colorFinder_green = ColorFinder(trackBar=False, colorSpace='HSV')  \n",
    "\n",
    "    current_annotations_green = {key: val for key, val in annotations.items() if 'green'  in key or 'grid' in key or 'start' in key}\n",
    "    mask_green = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "   \n",
    "    for polygon_name_green, points_green in current_annotations_green.items():\n",
    "        points_array_green = np.array(points_green, dtype=np.int32)\n",
    "        cv2.fillPoly(mask_green, [points_array_green], 255)\n",
    "    \n",
    "    masked_frame_green = cv2.bitwise_and(frame, frame, mask=mask_green)\n",
    "    masked_frame_green = cv2.bitwise_and(frame, frame, mask=mask_green)\n",
    "    imgColor_green, mask_green = colorFinder_green.update(masked_frame_green, vals_green)\n",
    "    imggray_green = cv2.cvtColor(imgColor_green, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_threshed_green = cv2.threshold(imggray_green, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations for green\n",
    "    kernel_green = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    img_threshed_green = cv2.morphologyEx(img_threshed_green, cv2.MORPH_OPEN, kernel_green)\n",
    "    kernel_green = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    img_threshed_green = cv2.morphologyEx(img_threshed_green, cv2.MORPH_CLOSE, kernel_green)\n",
    "    img_threshed_green = cv2.morphologyEx(img_threshed_green, cv2.MORPH_OPEN, kernel_green)\n",
    "    img_threshed_green = cv2.morphologyEx(img_threshed_green, cv2.MORPH_DILATE, kernel_green)\n",
    "\n",
    "    # Contour detection for green\n",
    "    contours_green, _ = cv2.findContours(img_threshed_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_green = sorted(contours_green, key=cv2.contourArea, reverse=True)[:4]\n",
    "    bounding_boxes_green = []\n",
    "    for cnt_green in contours_green:\n",
    "        area_green = cv2.contourArea(cnt_green)\n",
    "        if area_green > 50 : \n",
    "            perimeter_green = cv2.arcLength(cnt_green, True)\n",
    "            if perimeter_green == 0:\n",
    "                continue\n",
    "            circularity_green = 4 * np.pi * (area_green / (perimeter_green ** 2))\n",
    "            if circularity_green > 0.1:\n",
    "                x_green, y_green, w_green, h_green = cv2.boundingRect(cnt_green)\n",
    "                bounding_boxes_green.append((x_green, y_green, w_green, h_green))\n",
    "                cv2.rectangle(frame_annotated, (x_green, y_green), (x_green + w_green, y_green + h_green), (0, 255, 0), 2)\n",
    "\n",
    "    return frame_annotated,bounding_boxes_green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The detection of pawns and dice only\n",
    "\n",
    "here the experimenting with detecting the dice and the pawns take place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = cv2.VideoCapture('../videos/medium_3_Trim.mp4')\n",
    "current_annotations = transformed_annotations_6\n",
    "colorFinder = ColorFinder(False,'HLS')\n",
    "success, first_frame = video_2.read()\n",
    "if not success:\n",
    "    print(\"Failed to capture first frame. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Apply ColorFinder to get the mask based on the first frame\n",
    "vals_2 = {'hmin': 107, 'hmax': 140, 'smin': 0, 'smax': 255, 'lmin': 140, 'lmax': 255}\n",
    "imgColor, mask = colorFinder.update(first_frame, vals_2)\n",
    "mask_binary_white = cv2.inRange(mask, 1, 255)  \n",
    "mask_binary_white = cv2.morphologyEx(mask_binary_white, cv2.MORPH_DILATE, kernel, iterations=3)\n",
    "cv2.imshow(\"white\",mask_binary_white)\n",
    "cv2.waitKey(1)\n",
    "\n",
    "\n",
    "frame_counter = 1\n",
    "tracker = None\n",
    "tracking = False\n",
    "while True:\n",
    "    success_blue, frame_3 = video_2.read()\n",
    "    if not success_blue:\n",
    "        print(\"Failed to capture frame. Exiting...\")\n",
    "        break\n",
    "    \n",
    "    frame_annotated  = frame_3.copy()\n",
    "    frame_annotated,_ = detect_red_base(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,_ = detect_blue_base(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,_ = detect_blue_pawns(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,_ = detect_red_pawns(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,_ = detect_green_pawns(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,_ = detect_yellow_pawns(frame_3,current_annotations,frame_annotated)\n",
    "    frame_annotated,tracker,tracking = detect_dice(frame_3,frame_annotated,mask_binary_white,tracker,tracking,frame_counter)\n",
    "    frame_counter+=1\n",
    "    \n",
    "    if frame_counter %100 ==0:\n",
    "        current_annotations = allign_frame(frame_3,reference_pick)\n",
    "    cv2.imshow(\"annotated frame\",frame_annotated)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the detections  without the state game control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = cv2.VideoCapture('../videos/easy_1_Trim.mp4')\n",
    "current_annotations = transformed_annotations_1\n",
    "colorFinder = ColorFinder(False, 'HLS')\n",
    "\n",
    "success, first_frame = video_2.read()\n",
    "if not success:\n",
    "    print(\"Failed to capture first frame. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Apply ColorFinder to get the mask based on the first frame\n",
    "vals_2 = {'hmin': 107, 'hmax': 140, 'smin': 0, 'smax': 255, 'lmin': 140, 'lmax': 255}\n",
    "imgColor, mask = colorFinder.update(first_frame, vals_2)\n",
    "mask_binary_white = cv2.inRange(mask, 1, 255)  # Convert to binary mask\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "mask_binary_white = cv2.morphologyEx(mask_binary_white, cv2.MORPH_DILATE, kernel, iterations=4)\n",
    "cv2.imshow(\"white\", mask_binary_white)\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# Initialize variables for dice detection and tracking\n",
    "frame_counter = 1\n",
    "tracker = None\n",
    "tracking = False\n",
    "stable_count = 3\n",
    "dice_pips = None\n",
    "prev_bbox = None  # Initialize prev_bbox to track bounding box changes\n",
    "calculated = False  # Flag to track if pips have already been calculated\n",
    "\n",
    "# Main processing loop\n",
    "while True:\n",
    "    success_blue, frame_3 = video_2.read()\n",
    "    if not success_blue:\n",
    "        print(\"Failed to capture frame. Exiting...\")\n",
    "        break\n",
    "\n",
    "    frame_annotated = frame_3.copy()\n",
    "\n",
    "    # if frame_counter > 1200:\n",
    "        # Perform other detections\n",
    "    frame_annotated,_ = detect_red_base(frame_3, current_annotations, frame_annotated)\n",
    "    frame_annotated,_ = detect_blue_base(frame_3, current_annotations, frame_annotated)\n",
    "    frame_annotated,_ = detect_blue_pawns(frame_3, current_annotations, frame_annotated)\n",
    "    frame_annotated,_ = detect_red_pawns(frame_3, current_annotations, frame_annotated)\n",
    "    frame_annotated,_ = detect_green_pawns(frame_3, current_annotations, frame_annotated)\n",
    "    frame_annotated,_ = detect_yellow_pawns(frame_3, current_annotations, frame_annotated)\n",
    "\n",
    "    # Perform dice detection with pip counting\n",
    "    frame_annotated, tracker, tracking, stable_count, dice_pips, dice_img, prev_bbox, calculated = detect_dice_with_pips(\n",
    "        frame_3, frame_annotated, mask_binary_white, tracker, tracking, frame_counter, stable_count, dice_pips, prev_bbox, calculated\n",
    "    )\n",
    "    cv2.imshow(\"dice img\",dice_img)\n",
    "    if dice_pips is not None:\n",
    "            print(f'number of pips : {dice_pips}')\n",
    "            dice_pips = None\n",
    "\n",
    "    # Update annotations periodically\n",
    "    if frame_counter % 100 == 0:\n",
    "        current_annotations = allign_frame(frame_3, reference_pick)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"annotated frame\", frame_annotated)\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the game control\n",
    "\n",
    "Here all of the previous elements are gethered and the whole logic takes place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''the function responsible for decidin whether update the state on the game given the detections, and state of the game'''\n",
    "def update_grid_states_simple(detections, grid_states, annotations, overlap_threshold=0.1, area_threshold=0.3):\n",
    "    \n",
    "    updated_states = grid_states\n",
    "    grid_polygons = {key: Polygon(coords) for key, coords in annotations.items()}\n",
    "    for x, y, w, h in detections:\n",
    "        bbox = box(x, y, x + w, y + h)\n",
    "        best_grid_id = None\n",
    "        max_overlap = 0\n",
    "\n",
    "        for grid_id, polygon in grid_polygons.items():\n",
    "            intersection_area = bbox.intersection(polygon).area\n",
    "            bbox_area = bbox.area\n",
    "            overlap_ratio = intersection_area / bbox_area\n",
    "            tile_area = polygon.area\n",
    "            intersection_to_tile_ratio = intersection_area / tile_area\n",
    "\n",
    "            if overlap_ratio > max_overlap:\n",
    "                max_overlap = overlap_ratio\n",
    "                best_grid_id = grid_id\n",
    "\n",
    "            if intersection_to_tile_ratio >= area_threshold and 'base' in grid_id:\n",
    "                updated_states[grid_id] = True\n",
    "\n",
    "        if best_grid_id and max_overlap >= overlap_threshold:\n",
    "            updated_states[best_grid_id] = True\n",
    "\n",
    "    return updated_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function used for annotating the state of the game frame, basically the centers of the respective grid tiles are found \n",
    "for annotation'''\n",
    "def get_centroid(coords):\n",
    "    coords = np.array(coords)\n",
    "    x = coords[:, 0]\n",
    "    y = coords[:, 1]\n",
    "    centroid_x = int(np.mean(x))\n",
    "    centroid_y = int(np.mean(y))\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "'''the function to reset the state of detections after each frame'''\n",
    "def reset_detections(dictionary):\n",
    "    reseted_dictionary = {key:False for key in dictionary}\n",
    "    return reseted_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''getting the scaled annotations to be able to insert them on the refernce pick'''\n",
    "original_size = (1707*0.4, 1707*0.4)  \n",
    "resized_size = (512, 512)\n",
    "scaled_annotations = {}\n",
    "for key, polygons in loaded_annotations.items():\n",
    "    scaled_annotations[key] = [\n",
    "        [[int(pt[0] * resized_size[0] / original_size[0]), int(pt[1] * resized_size[1] / original_size[1])] for pt in polygon]\n",
    "        for polygon in polygons\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the history of the states of the grid tiles is updated, so basically there is some thershold of history for\n",
    "each grid tile stored, it stores True,False refering to either the tile was occupied or not\n",
    "the updating adds the most recent event to the history and in case the history is longer than threshold it erases the\n",
    "least recent event from the story'''\n",
    "def update_history(grid_history,grid_state):\n",
    "    \n",
    "    if len(list(grid_history.values())[0]) >=120:\n",
    "        for key,val in grid_state.items():\n",
    "            grid_history[key].append(val)\n",
    "            grid_history[key].pop(0)\n",
    "            \n",
    "    else:\n",
    "        for key,val in grid_state.items():\n",
    "            grid_history[key].append(val)\n",
    "            \n",
    "    return grid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''updating the changes , so the history of the changes of the grid states is also stored, and it helps determinging\n",
    "whether pawn moved or not '''\n",
    "def update_changes(grid_state,grid_history,grid_changes,stability_threshold=60,change_history=80,negative_stability=50):\n",
    "    # 40,50,30\n",
    "\n",
    "    key_to_change = None\n",
    "    for  key,val in  grid_state.items():\n",
    "        if val and not any(grid_history[key][-negative_stability:]):\n",
    "            grid_changes[key] =True\n",
    "        elif grid_changes[key]:\n",
    "            recent_history = grid_history[key][-change_history:]\n",
    "            if sum(recent_history) >= stability_threshold:\n",
    "                key_to_change = key \n",
    "                grid_changes[key] = False  \n",
    "                break\n",
    "    return grid_changes,key_to_change\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this function ensures that there are 4 pawns of the given color at given instance on the gird,\n",
    "it works in the following way. In case that less than 4 pawns are detected then we look in the history and\n",
    "find the most recent tile that has been occupied by the given color pawn but is not occupied now, and we infer the pawns\n",
    "is most likely there, only the contour has vanished due to shadowing occlucion etc.\n",
    "In case more than 4 pawns are detected than we do in different way, look at the detected tiles, and we want\n",
    "to dispose of the false  detections in bases, because they tend to be oversensitive - it is difficult to detect blue pawns\n",
    "of nearly the same hue as the tile it lays on and causes true positives. So we see the tiles which are now true but the \n",
    "last time they were true was longest ago'''\n",
    "def ensure_minimum_keys_true(grid_states, grid_history, min_true=4):\n",
    "    true_keys = [key for key, value in grid_states.items() if value]\n",
    "    num_true = len(true_keys)\n",
    "    \n",
    "\n",
    "    if num_true == min_true:\n",
    "        return grid_states\n",
    "    elif num_true > min_true:\n",
    "        keys_needed = num_true - min_true\n",
    "        candidate_keys = [\n",
    "        key for key, history in grid_history.items()\n",
    "        if  grid_states[key] and 'base' in key and not any(history) \n",
    "        ]\n",
    "        if candidate_keys is not None:\n",
    "            keys_needed = max(keys_needed - len(candidate_keys),0)\n",
    "            for key in candidate_keys[:keys_needed]:\n",
    "                grid_states[key] = False\n",
    "        if  keys_needed ==0:\n",
    "            return grid_states\n",
    "        candidate_keys = [\n",
    "        key for key, history in grid_history.items()\n",
    "        if  grid_states[key] and 'base' in key and any(history) \n",
    "        ]\n",
    "        candidate_keys.sort(key=lambda key: max(idx for idx, val in enumerate(grid_history[key]) if val))\n",
    "        for key in candidate_keys[:keys_needed]:\n",
    "            grid_states[key] = False\n",
    "        return grid_states\n",
    "\n",
    "    \n",
    "\n",
    "    else:\n",
    "        keys_needed = min_true - num_true\n",
    "        candidate_keys = [\n",
    "            key for key, history in grid_history.items()\n",
    "            if not grid_states[key] and any(history) \n",
    "        ]\n",
    "        \n",
    "        candidate_keys.sort(key=lambda key: max(idx for idx, val in enumerate(grid_history[key]) if val), reverse=True)\n",
    "        for key in candidate_keys[:keys_needed]:\n",
    "            grid_states[key] = True\n",
    "        \n",
    "        return grid_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tracking and  game state detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''here different annotations must be choosen for the different videos\n",
    "the annotations are not hardcoded but are calculated with the sift in the previous cells with respect to the\n",
    "annotated reference pick , '''\n",
    "video_2 = cv2.VideoCapture('../videos/hard_3.mp4')\n",
    "\n",
    "\n",
    "\n",
    "current_annotations = transformed_annotations_9\n",
    "colorFinder = ColorFinder(False, 'HLS')\n",
    "\n",
    "success, first_frame = video_2.read()\n",
    "if not success:\n",
    "    print(\"Failed to capture first frame. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "'''the white mask extraction from the first frame , to surpress the possibility of the false\n",
    "positive dice detections , as the filtering of the white dice is not possible with the white\n",
    "tape knife with the nearly the same parameters in each of the colorspaces: HSV, HLS, GBR'''\n",
    "vals_2 = {'hmin': 107, 'hmax': 140, 'smin': 0, 'smax': 255, 'lmin': 140, 'lmax': 255}\n",
    "imgColor, mask = colorFinder.update(first_frame, vals_2)\n",
    "mask_binary_white = cv2.inRange(mask, 1, 255)  \n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "mask_binary_white = cv2.morphologyEx(mask_binary_white, cv2.MORPH_DILATE, kernel, iterations=4)\n",
    "\n",
    "\n",
    "'''initialization of the all variables need to track the state of the game '''\n",
    "frame_counter = 1\n",
    "tracker = None\n",
    "tracking = False\n",
    "stable_count = 3\n",
    "dice_pips = None\n",
    "prev_bbox = None  #\n",
    "calculated = False  \n",
    "history_limit = 180\n",
    "last_dice_pips = 'not yet'\n",
    "pawn_movement = 'not yet moved'\n",
    "red_moved_to = None\n",
    "blue_moved_to = None\n",
    "green_moved_to = None\n",
    "yellow_moved_to = None\n",
    "movement_color = (255, 255, 255)\n",
    "entered_the_game = None\n",
    "\n",
    "\n",
    "'''initializing the historeis '''\n",
    "grid_histories_red = dict((k, [False]) if 'red_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "grid_histories_blue = dict((k, [False]) if 'blue_base' not in k  else (k, [True])  for k in current_annotations.keys())\n",
    "grid_histories_green = dict((k, [False]) if 'green_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "grid_histories_yellow = dict((k, [False]) if 'yellow_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "\n",
    "\n",
    "'''initializing the states '''\n",
    "grid_states_red = {key:False for key in current_annotations.keys()}\n",
    "grid_states_blue = {key: False for key in current_annotations.keys()}\n",
    "grid_states_green = {key: False for key in current_annotations.keys()}\n",
    "grid_states_yellow = {key: False for key in current_annotations.keys()}\n",
    "\n",
    "'''initializing the changes for the message of the pawns movement'''\n",
    "grid_changes_red = {key:False for key in current_annotations.keys()}\n",
    "grid_changes_blue = {key: False for key in current_annotations.keys()}\n",
    "grid_changes_green = {key: False for key in current_annotations.keys()}\n",
    "grid_changes_yellow = {key: False for key in current_annotations.keys()}\n",
    "\n",
    "'''initialized the ensured histories, which are the historeis but accounting for the correction from the \n",
    " ensure minimum keys ,so alwasy 4 detectons for each color '''\n",
    "\n",
    "grid_histories_red_ensured = dict((k, [False]) if 'red_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "grid_histories_blue_ensured = dict((k, [False]) if 'blue_base' not in k  else (k, [True])  for k in current_annotations.keys())\n",
    "grid_histories_green_ensured = dict((k, [False]) if 'green_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "grid_histories_yellow_ensured = dict((k, [False]) if 'yellow_base' not in k  else (k, [True]) for k in current_annotations.keys())\n",
    "\n",
    "\n",
    "\n",
    "'''changing the format of the annotations to be valid for the shapely polygons creation'''\n",
    "grid_polygons = {}\n",
    "for key, coords in current_annotations.items():\n",
    "    try:\n",
    "        grid_polygons[key] = current_annotations[key][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating polygon for {key} with coordinates {coords}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "'''the loop woth all the logic '''\n",
    "while True:\n",
    "    game_control_frame = reference_pick.copy()\n",
    "    success_blue, frame_3 = video_2.read()\n",
    "    if not success_blue:\n",
    "        print(\"Failed to capture frame. Exiting...\")\n",
    "        break\n",
    "\n",
    "    frame_annotated = frame_3.copy()\n",
    "    frame_tracked_state = frame_3.copy()\n",
    "\n",
    "    '''the detection only starts after some predefined times since firstly the pawns must be set on the grid '''\n",
    "    if frame_counter > 1: #1200\n",
    "        '''detect pawns of all the colors'''\n",
    "        frame_annotated, red_detections = detect_red_pawns(frame_3, current_annotations, frame_annotated)\n",
    "        frame_annotated, blue_detections = detect_blue_pawns(frame_3, current_annotations, frame_annotated)\n",
    "        frame_annotated, green_detections = detect_green_pawns(frame_3, current_annotations, frame_annotated)\n",
    "        frame_annotated, yellow_detections = detect_yellow_pawns(frame_3, current_annotations, frame_annotated)\n",
    "        frame_annotated,red_base_detections = detect_red_base(frame_3, current_annotations, frame_annotated)\n",
    "        frame_annotated,blue_base_detections= detect_blue_base(frame_3, current_annotations, frame_annotated)\n",
    "\n",
    "        '''Update grid states for each color'''\n",
    "        if red_base_detections:\n",
    "             grid_states_red = update_grid_states_simple(red_base_detections, grid_states_red, grid_polygons)\n",
    "        if blue_base_detections:\n",
    "             grid_states_blue = update_grid_states_simple(blue_base_detections, grid_states_blue, grid_polygons)\n",
    "        if red_detections:\n",
    "             grid_states_red = update_grid_states_simple(red_detections, grid_states_red, grid_polygons)\n",
    "        if blue_detections:\n",
    "            grid_states_blue = update_grid_states_simple(blue_detections, grid_states_blue, grid_polygons)\n",
    "        if yellow_detections:\n",
    "            grid_states_yellow = update_grid_states_simple(yellow_detections, grid_states_yellow, grid_polygons)\n",
    "        if green_detections:\n",
    "            grid_states_green = update_grid_states_simple(green_detections, grid_states_green, grid_polygons)\n",
    "\n",
    "        \n",
    "        '''updating the grid changes for each color '''\n",
    "        grid_changes_red,key_to_show_red = update_changes(grid_states_red,grid_histories_red_ensured,grid_changes_red)\n",
    "        grid_changes_blue,key_to_show_blue = update_changes(grid_states_blue,grid_histories_blue_ensured,grid_changes_blue)\n",
    "        grid_changes_green,key_to_show_green = update_changes(grid_states_green,grid_histories_green_ensured,grid_changes_green)\n",
    "        grid_changes_yellow,key_to_show_yellow = update_changes(grid_states_yellow,grid_histories_yellow_ensured,grid_changes_yellow)\n",
    "\n",
    "        '''updating the grid states accounting for the predefined number of pawns in each color -4 '''\n",
    "        grid_states_red_ensured = ensure_minimum_keys_true(grid_states_red,grid_histories_red)\n",
    "        grid_states_blue_ensured = ensure_minimum_keys_true(grid_states_blue,grid_histories_blue)\n",
    "        grid_states_green_ensured = ensure_minimum_keys_true(grid_states_green,grid_histories_green)\n",
    "        grid_states_yellow_ensured = ensure_minimum_keys_true(grid_states_yellow,grid_histories_yellow)\n",
    "\n",
    "        '''updated the historeies of the grid tiles'''\n",
    "        grid_histories_red = update_history(grid_histories_red,grid_states_red)\n",
    "        grid_histories_blue = update_history(grid_histories_blue,grid_states_blue)\n",
    "        grid_histories_green = update_history(grid_histories_green,grid_states_green)\n",
    "        grid_histories_yellow = update_history(grid_histories_yellow,grid_states_yellow)\n",
    "\n",
    "        '''updated the ensured historeis of the grid - accounting for the predefined number of pawns '''\n",
    "        grid_histories_red_ensured = update_history(grid_histories_red,grid_states_red_ensured)\n",
    "        grid_histories_blue_ensured = update_history(grid_histories_blue,grid_states_blue_ensured)\n",
    "        grid_histories_green_ensured = update_history(grid_histories_green,grid_states_green_ensured)\n",
    "        grid_histories_yellow_ensured = update_history(grid_histories_yellow,grid_states_yellow_ensured)\n",
    "\n",
    "\n",
    "        \n",
    "        '''checking if there was some change -pawns movement , if so the parameters for the message\n",
    "        to be displayed are set '''\n",
    "        if key_to_show_red and key_to_show_red != red_moved_to and 'red_base' not in key_to_show_red:\n",
    "            red_moved_to = key_to_show_red\n",
    "            pawn_movement = 'red pawn moved'\n",
    "            if key_to_show_red == 'red_start':\n",
    "                pawn_movement = 'red pawn entered the game'\n",
    "            movement_color = (0, 0, 255)\n",
    "        elif key_to_show_blue and key_to_show_blue != blue_moved_to and 'blue_base' not in key_to_show_blue:\n",
    "            blue_moved_to = key_to_show_blue\n",
    "            pawn_movement = 'blue pawn moved'\n",
    "            if key_to_show_blue == 'blue_start':\n",
    "                pawn_movement = 'blue pawn entered the game'\n",
    "            movement_color = (255, 0, 0)\n",
    "        elif key_to_show_yellow and key_to_show_yellow != yellow_moved_to and 'yellow_base' not in key_to_show_yellow:\n",
    "            yellow_moved_to = key_to_show_yellow\n",
    "            pawn_movement = 'yellow pawn moved'\n",
    "            if key_to_show_yellow == 'yellow_start':\n",
    "                pawn_movement = 'yellow pawn entered the game'\n",
    "            movement_color = (0, 255, 255)\n",
    "        elif key_to_show_green and key_to_show_green != green_moved_to and 'green_base' not in key_to_show_green:\n",
    "            green_moved_to = key_to_show_green\n",
    "            pawn_movement = 'green pawn moved'\n",
    "            if key_to_show_green == 'green_start':\n",
    "                pawn_movement = 'green pawn entered the game'\n",
    "            movement_color = (0, 255, 0)\n",
    "        \n",
    "        cv2.putText(game_control_frame, pawn_movement, (190,225), cv2.FONT_HERSHEY_SIMPLEX, 0.5, movement_color, 2)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        '''the detected position of the pawns are annotated on the grid'''\n",
    "        for grid_id, is_occupied in grid_states_red_ensured.items():\n",
    "            if is_occupied:\n",
    "                centroid = get_centroid(scaled_annotations[grid_id][0])\n",
    "                cv2.putText(game_control_frame, \"R\", centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        grid_states_red = reset_detections(grid_states_red)\n",
    "\n",
    "        for grid_id, is_occupied in grid_states_blue_ensured.items():\n",
    "            if is_occupied:\n",
    "                centroid = get_centroid(scaled_annotations[grid_id][0])\n",
    "                cv2.putText(game_control_frame, \"B\", centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        grid_states_blue = reset_detections(grid_states_blue)\n",
    "\n",
    "        for grid_id, is_occupied in grid_states_green_ensured.items():\n",
    "            if is_occupied:\n",
    "                centroid = get_centroid(scaled_annotations[grid_id][0])\n",
    "                cv2.putText(game_control_frame, \"G\", centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        grid_states_green = reset_detections(grid_states_green)\n",
    "\n",
    "        for grid_id, is_occupied in grid_states_yellow_ensured.items():\n",
    "            if is_occupied:\n",
    "                centroid = get_centroid(scaled_annotations[grid_id][0])\n",
    "                cv2.putText(game_control_frame, \"Y\", centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "        grid_states_yellow = reset_detections(grid_states_yellow)\n",
    "\n",
    "        \n",
    "\n",
    "        '''the dice is deteceted '''\n",
    "        frame_annotated, tracker, tracking, stable_count, dice_pips, dice_img, prev_bbox, calculated = detect_dice_with_pips(\n",
    "            frame_3, frame_annotated, mask_binary_white, tracker, tracking, frame_counter, stable_count, dice_pips, prev_bbox, calculated\n",
    "        )\n",
    "        if dice_pips is not None:\n",
    "            last_dice_pips = dice_pips\n",
    "            cv2.putText(game_control_frame, f'dice rolled: {last_dice_pips } rolled', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(game_control_frame, f'dice rolled: {last_dice_pips } rolled', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,(255, 255, 255) , 2)\n",
    "\n",
    "        '''the state of the game is displayed'''\n",
    "        cv2.imshow(\"inscribed frame\",game_control_frame)\n",
    "\n",
    "    '''the  frame must be periodically alligned  to account for an kind of shaking, grid movement camera movement\n",
    "    done every 100 frame to balance the qualitative  performance and the  computational complexity'''\n",
    "    if frame_counter % 200 == 0:\n",
    "        current_annotations = allign_frame(frame_3, reference_pick)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    '''the annotated frame is displayed '''\n",
    "    cv2.imshow(\"annotated frame\", frame_annotated)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
